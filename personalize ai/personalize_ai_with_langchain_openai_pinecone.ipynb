{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be98764",
   "metadata": {},
   "source": [
    "# Personalize AI with LangChain, OpenAI & Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e77d90",
   "metadata": {},
   "source": [
    "### Install and Import All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain\n",
    "!pip -q install openai\n",
    "!pip -q install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193889e0",
   "metadata": {},
   "source": [
    "### Tokenize Your Knowledge Base into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4baf9c",
   "metadata": {},
   "source": [
    "LangChain provides an extensive number of loaders and parsers for many document types. For simplicity we are hardcoding a markdown document from LangChain's readme.md file as our example knowledge base document and parsing it using it MarkdownTextSplitter, which splits text along Markdown headings, code blocks, or horizontal rules. We are using a maximum chunk size of 500, which works well for this type of content, but may not work as well for other types of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cd742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='# LangChain', metadata={}), Document(page_content='What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.', metadata={}), Document(page_content='The LangChain library is aimed at assisting in the development of those types of applications. There are six main areas that LangChain is designed to help with.', metadata={}), Document(page_content='LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\\n\\n### Chains:\\n\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.', metadata={}), Document(page_content='Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.', metadata={}), Document(page_content='Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.', metadata={}), Document(page_content='Memory:\\n\\nMemory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "markdown_text = \"\"\"\n",
    "# LangChain\n",
    "\n",
    "## What is this?\n",
    "\n",
    "Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not.\n",
    "But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\n",
    "\n",
    "The LangChain library is aimed at assisting in the development of those types of applications. There are six main areas that LangChain is designed to help with.\n",
    "\n",
    "### LLMs and Prompts:\n",
    "\n",
    "This includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\n",
    "\n",
    "### Chains:\n",
    "\n",
    "Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "### Data Augmented Generation:\n",
    "\n",
    "Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\n",
    "\n",
    "### Agents:\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
    "\n",
    "### Memory:\n",
    "\n",
    "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\"\"\"\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "chunks = markdown_splitter.create_documents([markdown_text])\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ea328",
   "metadata": {},
   "source": [
    "### Generate Embeddings for the Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08e5a9",
   "metadata": {},
   "source": [
    "We are using OpenAI to generate embeddings and LangChain's OpenAIEmbeddings class provides a wrapper around the OpenAI embedding model. To use this class, you must first get an API Key from OpenAI by going to [platform.openapi.com](https://platform.openai.com/account/api-keys) and paste it below. Note that API keys should not be hardcoded and instead loaded from the environment, but in this example we are just passing the api key as a parameter to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc20757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<class 'openai.api_resources.embedding.Embedding'> model='text-embedding-ada-002' document_model_name='text-embedding-ada-002' query_model_name='text-embedding-ada-002' embedding_ctx_length=8191 openai_api_key='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' openai_organization=None allowed_special=set() disallowed_special='all' chunk_size=1000 max_retries=6\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefe241",
   "metadata": {},
   "source": [
    "### Store the Embeddings in a Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5098e2",
   "metadata": {},
   "source": [
    "We are using Pinecone to store embeddings and you will also need an API key from them at [app.pinecone.io](https://app.pinecone.io). Again, we would normally set the API key in the environment and get it via `os.getenv('PINECONE_API_KEY')`, but for simplicity, we are hardcoding it. When you get your API key, their dashboard will show the environment where the API key is valid, which will be something like \"us-central1-gcp\" or \"us-west1-gcp\". Copy and paste the key and environment below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"\"\n",
    "PINECONE_ENV = \"\"\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b42299",
   "metadata": {},
   "source": [
    "After Pinecode is initialized, we create a new index for our chunks. We specify the number of dimensions to match the dimensions that the OpenAI embeddings API generates. Also, as of this writing, Pinecode is experiencing a surge in traffic and may returns errors or timeouts, so be patient if trying this on your own, and check your Pinecode dashboard after each operation, as the index may be created successfully in the platform even when you received an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276db9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kbtest-idx']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"kbtest-idx\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # create index only if it does not exist\n",
    "    pinecone.create_index(index_name, dimension=1536)\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c926cb1",
   "metadata": {},
   "source": [
    "Now we load the new index with our chunks using the OpenAI embedding model we created earlier. The from_documents() function returns the Pinecode vector store instance that we can use later in further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046072a",
   "metadata": {},
   "source": [
    "We can confirm that our chunks got stored in Pinecone by querying the index stats and confirming that the vector_count is 7, which is the number of chunks generated by the MarkdownTextSplitter. The same information is shown on their dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 7}},\n",
       " 'total_vector_count': 7}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name=index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74250c",
   "metadata": {},
   "source": [
    "### Generate User Query Embeddings and Find Relevant Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5d0ab",
   "metadata": {},
   "source": [
    "Now that the vector store has indexed our chunks, we can query it for the most relevant chunks that match the user query. To keep it simple, we are hardcoding the user query we will be sending to the LLM. We are first exposing the vector store as a retriever. Retriever is a generic LangChain interface that makes it easy to combine a vector store with language models. The interface exposes a get_relevant_documents method, which accepts a query and returns a list of relevant documents. The k argument specifies the maximum number of results to return, which in our example is 2, meaning we only want the top two most relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d854e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.', metadata={}),\n",
       " Document(page_content='LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\\n\\n### Chains:\\n\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.', metadata={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"How can you harness the real power of LLMs?\" # user query to send to the LLM\n",
    "retriever = store.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever.get_relevant_documents(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6074103",
   "metadata": {},
   "source": [
    "### Inject the Relevant Chunks into the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b902db",
   "metadata": {},
   "source": [
    "The real power of the retriever interface is that we can feed it directly into a chain and leverage the power of LangChain. In this example, we create a RetrievalQA chain, which is a special purpose chain for question-answering. This chain automatically retrieves the most relevant chunks of text from the retriever and feeds them to the language model as context. To create the chain, we specify the model to answer questions and the retriever (which wraps the vector store) to perform the similarity search on the user query. Here we use OpenAI as the model and Pinecone as the retriever.\n",
    "\n",
    "We also supply a chain type, which defines how context (the relevant chunks) is injected into the prompt. Here we use a stuff chain type, which means all the relevant chunks are fed to the model without regard for size. Note that stuff is the default setting, so it does not need to be specified, but we do it here for clarity. We use stuff in this example because we are limiting the number of chunks being injected to 2 and each chunk has a predefined maximum length of 500 (which we defined in the MarkdownTextSplitter), so we know we will not exceed the model's token limit. But a real application will require a lot more thought to ensure it does not exceed the max prompt length.\n",
    "\n",
    "Finally, note that when using the RetrievalQA chain, it is not necessary to call the get_relevant_documents() method of the retriever, as we did above, as this is automatically done under the hood when you execute the RetrievalQA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2609649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How can you harness the real power of LLMs?',\n",
       " 'result': ' You can harness the real power of LLMs by combining them with other sources of computation or knowledge, such as prompt management, prompt optimization, and common utilities for working with LLMs. Additionally, LangChain can provide a standard interface for chains and end-to-end chains for common applications.',\n",
       " 'source_documents': [Document(page_content='What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.', metadata={}),\n",
       "  Document(page_content='LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.\\n\\n### Chains:\\n\\nChains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.', metadata={})]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=OPENAI_API_KEY), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "qa({\"query\": user_query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
